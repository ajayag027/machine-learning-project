{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7hBIi_osiCS2",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "67NQN5KX2AMe",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJAYAG100/HOTEL-BOOKING-ANALYSIS/blob/main/ML_Submission_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  YES BANK STOCK CLOSING PRICE PREDICTION\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  AJAY AGRAWAL"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to analyze the impact of a fraud case involving Rana Kapoor on the stock prices of Yes Bank, a prominent bank in the Indian financial domain. The dataset used in this project consisted of monthly stock prices of Yes Bank since its inception, including closing, starting, highest, and lowest stock prices.\n",
        "\n",
        "To predict the stock's closing price, I developed three models namely Ridge_regression, Random Forest and XGBoost Regressor model was developed. The model was trained using the historical stock price data and various features such as mean of Open, High and Low faetures.Additional features were engineered by taking lags to capture the temporal trends and patterns in the data.The performance of the model was evaluated using metrics like RMSE (Root Mean Squared Error),adjustes R2 and R2 score. xGBoost regressor performed well among the three with high R2 score and adjusted R2.\n",
        "\n",
        "The analysis aimed to uncover any patterns or changes in stock prices related to the fraud case involving Rana Kapoor. The feature importance provided by the XGBoost model helped identify the key factors influencing the stock price.\n",
        "\n",
        "Overall, the project aimed to contribute to a better understanding of the relationship between the fraud case and Yes Bank's stock prices, and to explore the potential of predictive models in the financial domain. The findings and insights gained from this project can be utilized by investors, analysts, and decision-makers to make informed investment or business decisions related to Yes Bank's stock."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stockâ€™s closing price of the month.\n",
        "\n",
        "A Stock or share is a financial instrument that represents ownership in a company. Units of stock are called \"shares.\" Stocks are bought and sold predominantly on stock exchanges, though there can be private sales as well, and are the foundation of many individual investors' portfolios."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUSINESS OBJECTIVE**"
      ],
      "metadata": {
        "id": "0cB648H43YmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The ultimate business objective is to leverage the regression model to provide accurate predictions of the closing price of Yes Bank stock, enabling stakeholders to make well-informed investment decisions, manage risks effectively, optimize portfolios, Early warning systems to alert any fraud cases like Rana Kapoor and align investment strategies with financial goals.\n",
        "\n",
        "Steps involved are:-\n",
        "\n",
        "1.DATA PREPROCESSING\n",
        "\n",
        "2.DATA CLEANING\n",
        "\n",
        "3.DATA DUPLICATION\n",
        "\n",
        "4.HANDLING OUTLIERS\n",
        "\n",
        "5.FEATURE TRANSFORMATION\n",
        "\n",
        "6.EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "7.ENCODING OF CATEGORICAL COLUMNS\n",
        "\n",
        "8.ALGORITHMS:-\n",
        "\n",
        "a.Linear Regression b.Ridge Regression c.Random Forest Regressor d.XGBoost Regressor"
      ],
      "metadata": {
        "id": "NPhItBME26F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import (MinMaxScaler,StandardScaler)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.linear_model import (Lasso, Ridge,ElasticNet, LassoCV, RidgeCV, ElasticNetCV)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = pd.read_csv(\"/content/data_YesBank_StockPrices.csv\")"
      ],
      "metadata": {
        "id": "VaBbCNbo_71M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "Dataset"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.head()"
      ],
      "metadata": {
        "id": "VvMdVct6AjHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.tail()"
      ],
      "metadata": {
        "id": "0KDEzyhXAnRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "Dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.size"
      ],
      "metadata": {
        "id": "XQjppdWFA5oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "Dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(Dataset[Dataset.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "\n",
        "\n",
        "msno.bar(Dataset,figsize=(5,5))\n",
        "\n",
        "\n",
        "plt.title('Missing Data bar Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here -- There are 5 columns and 185 records in the given dataset.It includes monthly stock prices from July 2005 to November 2020. Among the 5 columns, 4 are independent variables (Date, open,high,low) and dependent variable(close). There are no duplicate values, null values or missing values in the data."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "Dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "Dataset.describe(include = 'all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.describe()"
      ],
      "metadata": {
        "id": "2n03wF4kD9SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date - Date of the record. It has monthly dates from July 2005 to November 2020. data type is object need to change into datetime\n",
        "\n",
        "Open- opening price of the share/stock\n",
        "\n",
        "High - Highest price of the share for that day\n",
        "\n",
        "Low - Lowest Price of the share for that day\n",
        "\n",
        "Close - Closing price of the share for that day"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in Dataset.columns.tolist():\n",
        "  print('unique values in',i,'are',Dataset[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "Dataset_copy = Dataset.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.head()"
      ],
      "metadata": {
        "id": "eJJHuIcjGMoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.isnull().sum()"
      ],
      "metadata": {
        "id": "b5EaHHVNGULn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Dataset_copy[Dataset_copy.duplicated()])"
      ],
      "metadata": {
        "id": "kQv3lQkaGjHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting datatypes of data from string to datatime\n",
        "from datetime import datetime\n",
        "Dataset_copy['Date']= pd.to_datetime(Dataset_copy['Date'].apply(lambda x: datetime.strptime(x,'%b-%y')))"
      ],
      "metadata": {
        "id": "EXVB1uK8HADs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.head()"
      ],
      "metadata": {
        "id": "MyidMaJTIqUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.info()"
      ],
      "metadata": {
        "id": "HOLhbfTDIw74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.describe()"
      ],
      "metadata": {
        "id": "JUG0SJP2I1NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col=Dataset_copy.columns.to_list()\n",
        "numerical_cols=col[1:]"
      ],
      "metadata": {
        "id": "ZzDgmXv0JAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols"
      ],
      "metadata": {
        "id": "EQiqGEN-JPea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in numerical_cols:\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.boxplot(Dataset_copy[column],orient = 'h')\n",
        "  plt.xlabel(column, fontsize=10)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "xknmEXG8JjJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the date and index\n",
        "Dataset_copy.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "UTCAgNNGK-nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.head()"
      ],
      "metadata": {
        "id": "Hra_e03ZLXpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separting the data\n",
        "independent_variables = Dataset_copy.columns.tolist()[:-1]\n",
        "dependent_variable = ['Close']\n",
        "\n",
        "print(independent_variables)\n",
        "print(dependent_variable)\n",
        "\n"
      ],
      "metadata": {
        "id": "VKKO7EElLipP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have changed datatype of Date variable to datetime. Remaining all variables are numerical. numerical variables are Open,High, Low and Close(dependent variable). There are no null values and duplicate values. Data is clean and ready for vizualization. Mean is higher in numerical columns which indicate possibility of skewness in the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(7,8))\n",
        "Dataset_copy['Close'].plot(color = 'b')\n",
        "plt.xlabel('year')\n",
        "plt.ylabel('closing price')\n",
        "plt.title('closing price yearly')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how closing price in each year."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight form seeing the plot, it is evident that after the fraud case in 2018, the closing price took a huge hit and dropped significantly"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes, because of the 2018 fraud the yes bank stock has suffered immensily from 2018. That should not happen in the future."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# dependent variable 'Close'\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.distplot(Dataset_copy['Close'],color = 'b')\n",
        "plt.title('Distibution of Dependent Variable')\n",
        "plt.xlabel('closing price')\n",
        "plt.axvline(Dataset_copy['Close'].mean(),color= 'yellow')\n",
        "plt.axvline(Dataset_copy['Close'].median(),color = 'red',linestyle = 'dashed')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the Dependent variable. distplot gives more accurate result."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the data is somewhat positively skewed(right skew)"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, by observing the chart we now know that the closing price will always change over a period of time and that too because of the fraud in 2018, data is skewed positively. transformation need to be applied which will result in better prediction of closing price.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#applying the transformation\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.distplot(np.log10(Dataset_copy['Close']),color =\"b\")\n",
        "plt.title('Distibution of Transformed Dependent Variable')\n",
        "plt.xlabel('closing price')\n",
        "plt.axvline(np.log10(Dataset_copy['Close']).mean(),color = 'green')\n",
        "plt.axvline(np.log10(Dataset_copy['Close']).median(),color = 'red',linestyle = 'dashed')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To the see distribution of y variable"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the log transformation is nearly look likes normal distribution, mean,median and almost same"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the data is normally distributed, it becomes easy to develop a good model and positive business impact"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# plotting of distribution of independent features\n",
        "plt.figure(figsize =(12,5))\n",
        "for i ,col in enumerate(independent_variables):\n",
        "   plt.subplot(1,3,i+1)\n",
        "   sns.distplot(Dataset_copy[col],color ='b')\n",
        "   plt.xlabel(col,fontsize = 10)\n",
        "   plt.axvline(Dataset_copy[col].mean(),color = 'black')\n",
        "   plt.axvline(Dataset_copy[col].median(),color = 'red',linestyle = 'dashed')\n",
        "\n",
        "plt.suptitle('Distribution of Independent Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution across all independent variable"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that all the independent variables are right skewed and transformation is required"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By observing above chart I came to know that transformation need to be applied on all independent variables which will essentially require for a good model"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#distribution of transfored independent variable\n",
        "plt.figure(figsize = (12,5))\n",
        "for i, col in enumerate (independent_variables):\n",
        "  plt.subplot(1,3,i+1)\n",
        "  sns.distplot(np.log10(Dataset_copy[col]),color = 'b')\n",
        "  plt.xlabel(col, fontsize =10)\n",
        "\n",
        "  plt.axvline(np.log10(Dataset_copy[col]).mean(),color = 'green')\n",
        "  plt.axvline(np.log10(Dataset_copy[col]).median(),color = 'red',linestyle = 'dashed')\n",
        "\n",
        "plt.suptitle('Distribution Of Transformed Independent Variable')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution across all the independent variables after transformed applied\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the log Transformation, the data of all indepedent variables closely follow normal distribution"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build best model if the data is normally distributed."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "!pip install mplfinance\n",
        "\n",
        "import mplfinance as mpf\n",
        "\n",
        "df_candle = Dataset_copy[['Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "mpf.plot(df_candle, type='candle', style='yahoo', title='Candlestick Chart')\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "candle strick are most widely used finance world with this chart i want to see that variable for this month."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above chart, there is no much deviation from the price shown by open, high, low."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it make sure that there is no huge variation in data"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "df_price = np.log(Dataset_copy[['Open','High','Low']])\n",
        "plt.figure(figsize = (8,6))\n",
        "df_price.boxplot()\n",
        "plt.xlabel('columns')\n",
        "plt.ylabel('price')\n",
        "plt.title('Box Plots Of Price Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the outliers exits or not in the independent variable"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After transformation applied, outliers appeared to be diminished"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outliers are not in  the present the data however the given the dataset that small"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# plotting of the independent varaiable aganist dependent varaiable and checking the corelation between them\n",
        "for col in independent_variables:\n",
        "  fig = plt.figure(figsize =(12,5))\n",
        "  ax = fig.gca()\n",
        "  feature = Dataset_copy[col]\n",
        "  label = Dataset_copy['Close']\n",
        "  correlation = feature.corr(label)\n",
        "  plt.scatter(x = feature, y = label)\n",
        "\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('Close vs ' + col + '- correlation: ' + str(round((correlation),6)))\n",
        "\n",
        "  z = np.polyfit(Dataset_copy[col],Dataset_copy['Close'],1)\n",
        "  y_= np.poly1d(z)(Dataset_copy[col])\n",
        "\n",
        "  plt.plot(Dataset_copy[col],y_,\"r--\",lw = 1)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to plot correlation to independent and dependent variable"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " all independent variables are highly correlated and linear in fashion to dependent variable."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when independent variables are highly correlated to y variable,"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# extract the date and closing price columns from the dataset\n",
        "dates = Dataset_copy.index\n",
        "closing_prices = Dataset_copy['Close']\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(dates,closing_prices)\n",
        "plt.xlabel('Dates')\n",
        "plt.ylabel('closing prices')\n",
        "plt.title('Trend Of Closing Prices Over Time ')\n",
        "plt.xticks(rotation =40)\n",
        "plt.grid('True')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  tread of the dates and closing prices over time"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the highest 2018 of the share price also increase"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend of the closing prices and time also 2018 also increase to the price 357 also increse and 2020 also be decreased 18."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize = (12,5))\n",
        "Correlation = Dataset_copy.corr()\n",
        "sns.heatmap(abs(Correlation), annot=True, cmap='coolwarm')\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To the identifiy the correlation between the variable"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is expected that all independent variables are correlated to each other because opening price, high price, low price will not vary much in the finance sector."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(Dataset_copy)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seaborn Pairplot allows us to plot pairwise relationships between variables within a dataset. It gives us in single large picture. This is used essentially to get to know about our data and how our target variable is related with the rest of the variables"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy"
      ],
      "metadata": {
        "id": "dZUCRrJNFJHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds1 = Dataset_copy.copy()"
      ],
      "metadata": {
        "id": "E2JDb60dFHYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1 : There is a significant difference in the mean closing prices between the first half (2005-2017) and second half (2018-2020) of the dataset.\n",
        "\n",
        "Statement 2 : There is a significant difference in the mean closing prices between months with high opening prices and month with low openng prices.\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (HO): There is no signiificant difference in the mean closing prices between the first half and second half of the dataset.\n",
        "\n",
        "Alternate Hypothesis (H1) : There is a significant difference in the mean closing prices between the first half and second half of the dataset.\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "first_half = ds1['Close'][ds1.index.year <= 2017]\n",
        "second_half = ds1['Close'][ds1.index.year >= 2018]\n",
        "\n",
        "# calculated the means and the standard deviations of the two halfs\n",
        "mean1 = np.mean(first_half)\n",
        "mean2 = np.mean(second_half)\n",
        "std1 = np.std(first_half)\n",
        "std2 = np.std(second_half)\n",
        "\n",
        "# calculted the sample size\n",
        "n1 = len(first_half)\n",
        "n2 = len(second_half)\n",
        "\n",
        "# calculated the standard error of the difference between means\n",
        "standard_error = np.sqrt((std1**2/n1)+(std2**2/n2))\n",
        "\n",
        "# calculated the z score\n",
        "z = (mean1- mean2)/ standard_error\n",
        "\n",
        "#calculated the p values (two tailed test)\n",
        "p_value = 2*(1-stats.norm.cdf(abs(z)))\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "  print(\"Reject the null hypothesis. There is the significant difference in mean.\")\n",
        "else:\n",
        "  print(\"Fail to the Reject the null hypothesis. There is no significant difference in mean.\" )"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z test i my data has more than 30 record."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My datset more than 30 records.and i can calulated mean and sd from  it."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is significant difference in the mean closing prices between months with high opening prices and months with low opening prices.\n",
        "\n",
        "Null hypothesis(HO): The mean closing prices in months with high opening prices are equal to or lower than the mean closing price in months with low opening prices.\n",
        "\n",
        "Alternate Hypothesis(HA): The mean closing price in months with high opening prices are higher than the mean closing prices in months low opening prices.\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "ds1['month'] = ds1.index.strftime('%Y-%m')\n",
        "monthly_data = ds1.groupby('month').agg({'Open' :'mean','Close':'mean'})\n",
        "\n",
        "thershold = monthly_data['Open'].median()\n",
        "\n",
        "high_opening_prices = monthly_data[monthly_data['Open'] > thershold]['Close']\n",
        "low_opening_prices = monthly_data[monthly_data['Open'] <= thershold]['Close']\n",
        "\n",
        "# calculated the sample statistics\n",
        "mean_high = np.mean(high_opening_prices)\n",
        "mean_low = np.mean(low_opening_prices)\n",
        "std_high = np.std(high_opening_prices)\n",
        "std_low = np.mean(low_opening_prices)\n",
        "n_high = len(high_opening_prices)\n",
        "n_low = len(low_opening_prices)\n",
        "\n",
        "#calculated the z statistics\n",
        "z_statistics = (mean_high - mean_low) / np.sqrt((std_high**2/n_high) + (std_low**2 /n_low))\n",
        "\n",
        "p_value = 1- stats.norm.cdf(z_statistics)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "# compare the p value with the significant level\n",
        "if p_value < alpha:\n",
        "  print(\"Reject the null hypothesis. There is the significant differnce in the mean closing prices.\")\n",
        "else:\n",
        "  print(\"Fail to reject the null hypothesis. There are the no significant differnce in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "high_opening_prices = ds1[ds1['Open'] > thershold]['Close']\n",
        "low_opening_prices = ds1[ds1['Open'] <= thershold]['Close']\n",
        "\n",
        "# perform independent  t- test\n",
        "t_statistic, p_value = stats.ttest_ind(high_opening_prices, low_opening_prices)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "# compare the p value  with the significant level\n",
        "if p_value < alpha:\n",
        "  print(\"Reject the null hypothesis . There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "  print(\"FAil to reject the null hypothesis . There is no significant difference in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "GxnQGOQROaye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of the t test and z test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As my data the sample of the data and not normally distributed  there is skewness involved.\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "Dataset_copy.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is the open, high, low, Close and the null values and int 64 data types."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# As all the independent variable  are highly correlated. i can a create a new feature by taking mean from the each regonation .\n",
        "\n",
        "Dataset_copy['Mean_OHL']= Dataset_copy[['Open','High','Low']].mean(axis=1)\n",
        "Dataset_copy.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the relationship b/w dependent variable and independent variable\n",
        "sns.lmplot(x='Mean_OHL', y = 'Close', fit_reg = True, data = Dataset_copy)"
      ],
      "metadata": {
        "id": "Dv0lIkYAXZNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.log10(Dataset_copy['Mean_OHL'])\n",
        "sns.distplot(x)"
      ],
      "metadata": {
        "id": "AAEsEdSBYKst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (1,15):\n",
        "  Dataset_copy[\"lag_{}\".format(i)]= Dataset_copy.Mean_OHL.shift(i)"
      ],
      "metadata": {
        "id": "PXjswwbrYdth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_copy.head()"
      ],
      "metadata": {
        "id": "IDuuygDYZMXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "y_depend = Dataset_copy.dropna().Close.values\n",
        "x_independ = Dataset_copy.dropna().drop(['Close','Open','High','Low'],axis = 1)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_variable"
      ],
      "metadata": {
        "id": "pi97o6Vqabk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature of the selection to the provided to the x depend variable and y independ variable to the provide to the dependent variable of values."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open, high, Close I Think they must be included because on opening price is high and when low .There is significant change in closing prices."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "x_independ.head()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_independ['Mean_OHL'] = np.log10(x_independ['Mean_OHL'])\n",
        "y = np.log10(y_depend)\n",
        "x_independ.values"
      ],
      "metadata": {
        "id": "0tJHWKHJbvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "Scaler = StandardScaler()\n",
        "x_Scaled =Scaler.fit_transform(x_independ.values)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The to scaler functiion of the transform values to the provided to the values."
      ],
      "metadata": {
        "id": "tW21h53CdOIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "price range: Insted of the using open and high, low separately. you can the calculated the price range the differnce between the high and low prices.\n",
        "\n",
        "Average price : Another apporch is to be calculate the average price of average open , high and low prices. this varaible represent the overall price ."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "x_train, x_test,y_train, y_test = train_test_split(x_Scaled,y, test_size = 0.3,random_state = 1)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have to 80 percent of the  training data  and 20 percent of the testing of the data ."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i do not think the data in imbalanced of the data ."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are the  implementing basic linear regression model\n",
        "\n",
        "The linear regression model assume that the relationship between the dependent variable and independent variable is linear.\n",
        "which means that the change in the dependent variable is proportional to the change in the independent variable .\n",
        "\n",
        "during the training process. The linear regression tries to find that the values of the cofficient that the minimize the sum of the squared."
      ],
      "metadata": {
        "id": "bpTuaqVkiWFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "reg_with_transformation = LinearRegression().fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_with_transformation.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "V7jNiglhlEQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_with_transformation = reg_with_transformation.predict(x_train)\n",
        "y_test_pred_with_transformation = reg_with_transformation.predict(x_test)"
      ],
      "metadata": {
        "id": "2kGIcpJLlZtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparision_trans = pd.DataFrame(zip(10**(y_test), 10**(y_test_pred_with_transformation)), columns = ['actual', 'pred'])\n",
        "comparision_trans.head()"
      ],
      "metadata": {
        "id": "o3aijkh4mK0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_MAE = mean_absolute_error(10**(y_train),(10**y_train_pred_with_transformation))\n",
        "print(f\"Mean Absoluted Error :{train_MAE}\")\n",
        "\n",
        "train_MSE = mean_squared_error(10**(y_train),10**(y_train_pred_with_transformation))\n",
        "print(\"MSE:\",train_MSE)\n",
        "\n",
        "train_RMSE = np.sqrt(train_MSE)\n",
        "print(\"RMSE :\",train_RMSE)\n",
        "\n",
        "train_r2 = r2_score(10**(y_train), 10**(y_train_pred_with_transformation))\n",
        "print(\"R2:\",train_r2)\n",
        "\n",
        "MAE = mean_absolute_error(10**(y_test),(10**y_test_pred_with_transformation))\n",
        "print(f\"Mean Absolute Error : {MAE}\")\n",
        "\n",
        "MSE = mean_squared_error(10**(y_test),10**(y_test_pred_with_transformation))\n",
        "print(\"MSE:\",MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE:\",RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test),10**(y_test_pred_with_transformation))\n",
        "print(\"R2:\",r2)\n"
      ],
      "metadata": {
        "id": "M76ST7KSncZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MAE','MSE','RMSE','R2']\n",
        "scores = ['MAE','MSE','RMSE','r2']\n",
        "\n",
        "plt.figure(figsize =(9,8))\n",
        "plt.bar(metrics,scores)\n",
        "plt.xlabel('Regulation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Score Chart For Linear Regression With Transformation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing actual and predicated data\n",
        "\n",
        "fig, (ax1) =plt.subplots(1,1, figsize = (10,7))\n",
        "ax1.plot(10**(y_test_pred_with_transformation))\n",
        "ax1.plot(np.array(10**(y_test)))\n",
        "ax1.legend([\"Predicted\",\"Actual\"])\n",
        "ax1.set_title(\"predicted vs Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zfg35yrQ2upz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize =(9,7))\n",
        "sns.distplot((10**(y_test)-10**(y_test_pred_with_transformation)),bins =20)\n",
        "fig.suptitle(\"Residual Analysis\",fontsize = 20)"
      ],
      "metadata": {
        "id": "yOXPkrYi2umR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = 10**(y_test)-10**(y_test_pred_with_transformation)\n",
        "plt.scatter(10**(y_test_pred_with_transformation),residuals)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tbg-cRsN5pfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regessor_list = {'Train Mean Absolate Error':train_MAE,'Train Mean Squared Error':train_MSE,'Train Root Mean squared Error':train_RMSE,'Train R2 Score':train_r2,'Mean Absolute Error':MAE,'Mean Squared Error':MSE,'Root Mean Squared Error':RMSE,'R2 Score':r2}\n",
        "metrics = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()\n",
        "metrics = metrics.rename(columns={'index':'Metric',0:'reg_with_transformation'})\n",
        "metrics"
      ],
      "metadata": {
        "id": "-mSYKqAL6p-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root Mean squared Error(RMSE) is slightly higher on the test set(11.256322)\n",
        "compare to the training set (14.271363) indicating the slightly larger average mangnitude error in the predicating the close values on the test set.\n",
        "\n",
        "The R2 score slightly lower on the test set (0.973258) compare to the training set (0.982478), suggesting that the model explain a slightly lower proportion in the close variance in the close variable on test set.\n",
        "\n",
        "overall  the model perform well on the both the training and test set, but there is slightly higher level of the error slightly lower explantory power on the test set."
      ],
      "metadata": {
        "id": "PoW8RMM0-FfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameter = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False],\n",
        "\n",
        "    'positive': [True, False]\n",
        "}\n",
        "\n",
        "# Create the grid search object\n",
        "Lr_gs=GridSearchCV(reg_with_transformation,param_grid=parameter,cv=5,scoring='r2')\n",
        "\n",
        "# Fit the Algorithm\n",
        "Lr_gs.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_gs=Lr_gs.predict(x_test)\n",
        "y_pred_train_gs=Lr_gs.predict(x_train)"
      ],
      "metadata": {
        "id": "ctXz5DCFKvc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric score for the train test\n",
        "train_MAE_gs = mean_absolute_error(10**(y_train),(10**y_pred_train_gs))\n",
        "print(f\"Mean Absolute Error :(train_MAE_gs)\")\n",
        "\n",
        "train_MSE_gs = mean_squared_error(10**(y_train),10**(y_pred_train_gs))\n",
        "print(\"MSE:\",train_MSE_gs)\n",
        "\n",
        "train_RMSE_gs = np.sqrt(train_MSE_gs)\n",
        "print(\"RMSE:\",train_RMSE_gs)\n",
        "\n",
        "train_r2_gs = r2_score(10**(y_train),10**(y_pred_train_gs))\n",
        "print(\"R2:\",train_r2_gs)\n",
        "\n",
        "#metric for the test set\n",
        "MAE_gs = mean_absolute_error(10**(y_test),(10**y_pred_test_gs))\n",
        "print(f\"Mean Absolute Error :(MAE_gs)\")\n",
        "\n",
        "MSE_gs = mean_squared_error(10**(y_test),10**(y_pred_test_gs))\n",
        "print(\"MSE:\",MSE_gs)\n",
        "\n",
        "RMSE_gs = np.sqrt(MSE_gs)\n",
        "print(\"RMSE:\",RMSE_gs)\n",
        "\n",
        "r2_gs = r2_score(10**(y_test), 10**(y_pred_test_gs))\n",
        "print(\"R2 :\" ,r2_gs)\n",
        "\n"
      ],
      "metadata": {
        "id": "3TWvmBKnK9Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE and RMSE values for the test set are lower than those for the train set. indicatting performance on the test set.\n",
        "\n",
        "The R2 score for the test set is slightly higher than for the train set , suggesting  that the model generlized  well to unseen data .\n",
        "\n",
        "overall for model shows good performance on the both of train and test set. with low and R2 and high R2 score  to overcome of we can apply reguralation techniques ."
      ],
      "metadata": {
        "id": "8UdGS5EBdQa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot with transformation\n",
        "plt.plot(10 ** (y_pred_test_gs))\n",
        "plt.plot(np.array(10 ** (y_test)))\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "plt.title(\"Predicted vs Actual (with Transformation)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xz6U_V05BIVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(10,10))\n",
        "\n",
        "sns.distplot((10**(y_test)- 10**(y_pred_test_gs)),bins=20)\n",
        "\n",
        "#Plot Label\n",
        "fig.suptitle('Residual Analysis', fontsize = 20)"
      ],
      "metadata": {
        "id": "Msc4Yy5ABkvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Homoscadasticity\n",
        "residuals = 10**(y_test)-10**(y_pred_test_gs)\n",
        "\n",
        "# Plot the residuals against the predicted values\n",
        "plt.scatter( 10**(y_pred_test_gs),residuals)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4IRZ1BABxn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(residuals)"
      ],
      "metadata": {
        "id": "7E4EgY4tB2ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the assumptions of Linear Regression is being taken care. Mean of Residuals is nearly zero, and there is no hetroscadasticity."
      ],
      "metadata": {
        "id": "IKi9SvcDB9Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics['Lr_gs'] = [train_MAE_gs, train_MSE_gs, train_RMSE_gs, train_r2_gs, MAE_gs,MSE_gs,RMSE_gs,r2_gs,]"
      ],
      "metadata": {
        "id": "DC2JS9d6CEXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "In GridSearchCV,cross-validation is also performed which is used while training the model"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics\n"
      ],
      "metadata": {
        "id": "aPyJPOsnCerG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model has improved in terms of predicting the unseen data as the MAE,RMSE are lower than simple regression model and R2  has increased after cross validation and hyper parameter tuning."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression (L2 Regularization):\n",
        "\n",
        "Ridge adds a penalty term to the loss function that shrinks the coefficients towards zero without eliminating them completely\n",
        "\n",
        "\n",
        "I am using this because, I do not want to eliminate my features by using L1 or Elasticnet, as I included Lag values to identify the past trends to predict more accurately.\n",
        "\n",
        "Ridge is generally more suitable when all the features are expected to contribute to the model and no feature selection is desired."
      ],
      "metadata": {
        "id": "X5ct6JNpC3GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RIDGE with Cross Validation and Hyper parameter tuning"
      ],
      "metadata": {
        "id": "2pTfOYlXDEr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge Regularization\n",
        "ridge = Ridge()\n",
        "parameters2 = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,75,80,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters2, scoring='r2', cv=10)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "XB3ENLGNC_J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" r2 score is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "DLKBQWLADTV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ridge_regressor)"
      ],
      "metadata": {
        "id": "0nWUlTL4DWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ridge_model1 = ridge_regressor.best_estimator_\n",
        "\n",
        "print(best_ridge_model1.coef_)\n",
        "print(best_ridge_model1.intercept_)"
      ],
      "metadata": {
        "id": "fWxv5sKVDiNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_ridge = ridge_regressor.predict(x_train)\n",
        "y_test_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "JnKC3nAADm_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_regressor.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "6Cu4n1-KDsme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_ridge = mean_absolute_error(10**(y_train),(10**y_train_pred_ridge))\n",
        "print(f\"Mean Absolute Error : {train_MAE_ridge}\")\n",
        "\n",
        "\n",
        "train_MSE_ridge  = mean_squared_error(10**(y_train), 10**(y_train_pred_ridge))\n",
        "print(\"MSE :\" , train_MSE_ridge)\n",
        "\n",
        "train_RMSE_ridge = np.sqrt(train_MSE_ridge)\n",
        "print(\"RMSE :\" ,train_RMSE_ridge)\n",
        "\n",
        "train_r2_ridge = r2_score(10**(y_train), 10**(y_train_pred_ridge))\n",
        "print(\"R2 :\" ,train_r2_ridge)\n",
        "\n",
        "train_adjusted_r2_ridge=1-(1-r2_score(10**(y_train), 10**(y_train_pred_ridge)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_ridge)\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_ridge = mean_absolute_error(10**(y_test),(10**y_test_pred_ridge))\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = mean_squared_error(10**(y_test), 10**(y_test_pred_ridge))\n",
        "print(\"MSE :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = np.sqrt(MSE_ridge)\n",
        "print(\"RMSE :\" ,RMSE_ridge)\n",
        "\n",
        "r2_ridge = r2_score(10**(y_test), 10**(y_test_pred_ridge))\n",
        "print(\"R2 :\" ,r2_ridge)\n",
        "\n",
        "adjusted_r2_ridge=1-(1-r2_score(10**(y_test), 10**(y_test_pred_ridge)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_ridge)"
      ],
      "metadata": {
        "id": "2RQadRoyDzzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE has been reduced in the dataset which indicates model and predicting good on unseen data RMSE is also similar both cases .\n",
        "\n",
        "overall the model shows good performance on the both train and test dataset with relatively the low error and high to R2 values ."
      ],
      "metadata": {
        "id": "-CPPUaKfD9jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot with transformation\n",
        "plt.plot(10 ** (y_test_pred_ridge))\n",
        "plt.plot(np.array(10 ** (y_test)))\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "plt.title(\"Predicted vs Actual \")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PsCDLfAmFIqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(8,8))\n",
        "\n",
        "sns.distplot((10**(y_test)- 10**(y_test_pred_ridge)),bins=20)\n",
        "\n",
        "#Plot Label\n",
        "fig.suptitle('Residual Analysis', fontsize = 20)"
      ],
      "metadata": {
        "id": "dRI6VaB1FPQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Homoscadasticity\n",
        "residuals = 10**(y_test)-10**(y_test_pred_ridge)\n",
        "\n",
        "# Plot the residuals against the predicted values\n",
        "plt.scatter( 10**(y_test_pred_ridge),residuals)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dDXcgKCYFX1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics['ridge_regressor'] = [train_MAE_ridge, train_MSE_ridge, train_RMSE_ridge, train_r2_ridge, MAE_ridge,MSE_ridge,RMSE_ridge,r2_ridge]"
      ],
      "metadata": {
        "id": "W3kVc7f2Fjuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "b9_NCoX4GrF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has been has improved compared to pervious model as it is producting leat MAE, and also the handling overfitting problem which be faced with second problem .\n"
      ],
      "metadata": {
        "id": "hMukYB7CHEzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForest model\n",
        "Random forest is an ensamble learning algorithm that a constructs a multitude of decision trees at training time output mean the prediction of the individual tree as the final prediction\n",
        "\n",
        "The RaandomforestRegressor class allows to the train a regression model using the random forest algorthim . and then to use it to a make prediction on the data .\n",
        "\n",
        "The helps to reduce overfitting and improve the generation performance of the model."
      ],
      "metadata": {
        "id": "T5kCY3wzH9eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_rf =rf.predict(x_train)\n",
        "y_pred_test_rf =rf.predict(x_test)"
      ],
      "metadata": {
        "id": "h4B96LErKHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Metric Score for train set\n",
        "train_MAE_rf = mean_absolute_error(10**(y_train),(10**y_pred_train_rf))\n",
        "print(f\"Mean Absolute Error : {train_MAE_rf}\")\n",
        "\n",
        "\n",
        "train_MSE_rf  = mean_squared_error(10**(y_train), 10**(y_pred_train_rf))\n",
        "print(\"MSE :\" , train_MSE_rf)\n",
        "\n",
        "train_RMSE_rf = np.sqrt(train_MSE_rf)\n",
        "print(\"RMSE :\" ,train_RMSE_rf)\n",
        "\n",
        "train_r2_rf = r2_score(10**(y_train), 10**(y_pred_train_rf))\n",
        "print(\"R2 :\" ,train_r2_rf)\n",
        "\n",
        "train_adjusted_r2_rf=1-(1-r2_score(10**(y_train), 10**(y_pred_train_rf)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_rf)\n",
        "\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_rf = mean_absolute_error(10**(y_test),(10**y_pred_test_rf))\n",
        "print(f\"Mean Absolute Error : {MAE_rf}\")\n",
        "\n",
        "MSE_rf  = mean_squared_error(10**(y_test), 10**(y_pred_test_rf))\n",
        "print(\"MSE :\" , MSE_rf)\n",
        "\n",
        "RMSE_rf = np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\" ,RMSE_rf)\n",
        "\n",
        "r2_rf = r2_score(10**(y_test), 10**(y_pred_test_rf))\n",
        "print(\"R2 :\" ,r2_rf)\n",
        "\n",
        "adjusted_r2_rf=1-(1-r2_score(10**(y_test), 10**(y_pred_test_rf)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_rf)"
      ],
      "metadata": {
        "id": "I0YXJ2j6KKM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_pred_test_rf)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F2d9qMiqKuJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "rjqARt7wK9JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators' :[50,80,100,200,300],\n",
        "    'max_depth': [1,2,6,7,8,9,10,20,30,40],\n",
        "    'min_samples_split':[10,20,30,40,50,100,150,200],\n",
        "    'min_samples_leaf': [1,2,8,10,20,40,50]\n",
        "    }\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(rf, param_grid_rf,verbose=3, cv=7, scoring='r2')\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_rf_rs = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "O0xsv8cYK3s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_rf_rs.feature_importances_"
      ],
      "metadata": {
        "id": "Mw8PU7zWM0dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model_rf_rs)"
      ],
      "metadata": {
        "id": "I9LN6QCRNe8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the model\n",
        "y_pred_train_rf_rs= random_search.predict(x_train)\n",
        "y_pred_test_rf_rs= random_search.predict(x_test)"
      ],
      "metadata": {
        "id": "nf1QBO1WOAi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "e3rkvrHPOHz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric score for train set\n",
        "train_MAE_rf_rs = mean_absolute_error(10**(y_train),(10**y_pred_train_rf_rs))\n",
        "print(f\"Mean Absolute Error : {train_MAE_rf_rs}\")\n",
        "\n",
        "\n",
        "train_MSE_rf_rs  = mean_squared_error(10**(y_train), 10**(y_pred_train_rf_rs))\n",
        "print(\"MSE :\" , train_MSE_rf_rs)\n",
        "\n",
        "train_RMSE_rf_rs = np.sqrt(train_MSE_rf_rs)\n",
        "print(\"RMSE :\" ,train_RMSE_rf_rs)\n",
        "\n",
        "train_r2_rf_rs = r2_score(10**(y_train), 10**(y_pred_train_rf_rs))\n",
        "print(\"R2 :\" ,train_r2_rf_rs)\n",
        "\n",
        "train_adjusted_r2_rf_rs=1-(1-r2_score(10**(y_train), 10**(y_pred_train_rf_rs)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_rf_rs)\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_rf_rs = mean_absolute_error(10**(y_test),(10**y_pred_test_rf_rs))\n",
        "print(f\"Mean Absolute Error : {MAE_rf_rs}\")\n",
        "\n",
        "MSE_rf_rs  = mean_squared_error(10**(y_test), 10**(y_pred_test_rf_rs))\n",
        "print(\"MSE :\" , MSE_rf_rs)\n",
        "\n",
        "RMSE_rf_rs = np.sqrt(MSE_rf_rs)\n",
        "print(\"RMSE :\" ,RMSE_rf_rs)\n",
        "\n",
        "r2_rf_rs = r2_score(10**(y_test), 10**(y_pred_test_rf_rs))\n",
        "print(\"R2 :\" ,r2_rf_rs)\n",
        "\n",
        "adjusted_r2_rf_rs=1-(1-r2_score(10**(y_test), 10**(y_pred_test_rf_rs)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_rf_rs)"
      ],
      "metadata": {
        "id": "FBdYyv63Oj1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_pred_test_rf_rs)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i9phKTBNO4M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics['random_search'] = [train_MAE_rf_rs, train_MSE_rf_rs, train_RMSE_rf_rs, train_r2_rf_rs,MAE_rf_rs,MSE_rf_rs,RMSE_rf_rs,r2_rf_rs]"
      ],
      "metadata": {
        "id": "vfhk_HkZPCQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "0_umR3lnPZUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using the cross validation and hyper parameter tuning .the model has improved by overcoming and overfitting model."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost model\n",
        "\n",
        "\n",
        "It is a popular machine learning algorithm that uses an ensemble of decision trees to make predictions\n",
        "\n",
        "\n",
        "The XGBRegressor class allows us to train a regression model using the XGBoost algorithm which is then used to make predictions on new data.\n",
        "\n",
        "\n",
        "The model is trained by fitting a sequence of decision trees to the training data, with each new tree trying to correct the errors of the previous trees."
      ],
      "metadata": {
        "id": "oR23TFXYRYEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "xgboost = XGBRegressor(objective = 'reg:squarederror')\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgboost.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xg = xgboost.predict(x_train)\n",
        "y_pred_test_xg = xgboost.predict(x_test)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_xg = mean_absolute_error(10**(y_train),(10**y_pred_train_xg))\n",
        "print(f\"Mean Absolute Error : {train_MAE_xg}\")\n",
        "\n",
        "\n",
        "train_MSE_xg  = mean_squared_error(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"MSE :\" , train_MSE_xg)\n",
        "\n",
        "train_RMSE_xg = np.sqrt(train_MSE_xg)\n",
        "print(\"RMSE :\" ,train_RMSE_xg)\n",
        "\n",
        "train_r2_xg = r2_score(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"R2 :\" ,train_r2_xg)\n",
        "\n",
        "train_adjusted_r2_xg=1-(1-r2_score(10**(y_train), 10**(y_pred_train_xg)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_xg)\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_xg = mean_absolute_error(10**(y_test),(10**y_pred_test_xg))\n",
        "print(f\"Mean Absolute Error : {MAE_xg}\")\n",
        "\n",
        "MSE_xg  = mean_squared_error(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"MSE :\" , MSE_xg)\n",
        "\n",
        "RMSE_xg = np.sqrt(MSE_xg)\n",
        "print(\"RMSE :\" ,RMSE_xg)\n",
        "\n",
        "r2_xg = r2_score(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"R2 :\" ,r2_xg)\n",
        "\n",
        "adjusted_r2_xg=1-(1-r2_score(10**(y_test), 10**(y_pred_test_xg)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_xg)"
      ],
      "metadata": {
        "id": "nvyH14RcTAt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#Converting into readable format\n",
        "EM=['MAE','MSE','RMSE','r2','adjusted_r2']\n",
        "train_xg=[train_MAE_xg,train_MSE_xg,train_RMSE_xg,train_r2_xg,train_adjusted_r2_xg]\n",
        "test_xg=[MAE_xg,MSE_xg,RMSE_xg,r2_xg,adjusted_r2_xg]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_xg=pd.DataFrame({'Evalution Parameters': EM, 'Train':train_xg, 'Test':test_xg}).set_index('Evalution Parameters')\n",
        "data_xg"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is completely learning the data including noise. it is not performing well on the test data. overfitting the train dataset."
      ],
      "metadata": {
        "id": "cOZckxEJT2aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,7))\n",
        "plt.plot(10**y_pred_test_xg)\n",
        "plt.plot(np.array((10**y_test)))\n",
        "plt.legend(['Predicted', 'Actual'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bb2SQAZjUsGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid_xg = {\n",
        "    'n_estimators': [50,80,100,200,300],\n",
        "    'max_depth': [2,4,6,8],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "xgb_gs = GridSearchCV(estimator=xgboost, param_grid=param_grid_xg, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "xgb_gs.fit(x_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_xgb_gs = xgb_gs.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xgb_gs= xgb_gs.predict(x_train)\n",
        "y_pred_test_xgb_gs= xgb_gs.predict(x_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_xgb_gs.feature_importances_"
      ],
      "metadata": {
        "id": "3dQs0-Caeehp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model_xgb_gs)"
      ],
      "metadata": {
        "id": "DbsZ7sB8ev3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_gs.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "FImc4Lj6e4-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_xgb_gs = mean_absolute_error(10**(y_train),(10**y_pred_train_xgb_gs))\n",
        "print(f\"Mean Absolute Error : {train_MAE_xgb_gs}\")\n",
        "\n",
        "\n",
        "train_MSE_xgb_gs  = mean_squared_error(10**(y_train), 10**(y_pred_train_xgb_gs))\n",
        "print(\"MSE :\" , train_MSE_xgb_gs)\n",
        "\n",
        "train_RMSE_xgb_gs = np.sqrt(train_MSE_xgb_gs)\n",
        "print(\"RMSE :\" ,train_RMSE_xgb_gs)\n",
        "\n",
        "train_r2_xgb_gs = r2_score(10**(y_train), 10**(y_pred_train_xgb_gs))\n",
        "print(\"R2 :\" ,train_r2_xgb_gs)\n",
        "\n",
        "train_adjusted_r2_xgb_gs=1-(1-r2_score(10**(y_train), 10**(y_pred_train_xgb_gs)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_xgb_gs)\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_xgb_gs = mean_absolute_error(10**(y_test),(10**y_pred_test_xgb_gs))\n",
        "print(f\"Mean Absolute Error : {MAE_xgb_gs}\")\n",
        "\n",
        "MSE_xgb_gs  = mean_squared_error(10**(y_test), 10**(y_pred_test_xgb_gs))\n",
        "print(\"MSE :\" , MSE_xgb_gs)\n",
        "\n",
        "RMSE_xgb_gs = np.sqrt(MSE_xgb_gs)\n",
        "print(\"RMSE :\" ,RMSE_xgb_gs)\n",
        "\n",
        "r2_xgb_gs = r2_score(10**(y_test), 10**(y_pred_test_xgb_gs))\n",
        "print(\"R2 :\" ,r2_xgb_gs)\n",
        "\n",
        "adjusted_r2_xgb_gs=1-(1-r2_score(10**(y_test), 10**(y_pred_test_xgb_gs)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_xgb_gs)"
      ],
      "metadata": {
        "id": "G_O0nON_fL12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_pred_test_xgb_gs)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a8xVTQcifj5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics['xgb_gs'] = [train_MAE_xgb_gs, train_MSE_xgb_gs, train_RMSE_xgb_gs, train_r2_xgb_gs,MAE_xgb_gs,MSE_xgb_gs,RMSE_xgb_gs,r2_xgb_gs]"
      ],
      "metadata": {
        "id": "gcDA5ZG4fpB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used to GridSearchCV hyperparameter optimization technique which use grid search technique for finding the optimal hyperparameter to increase the model performance .\n",
        "\n",
        "in GridSearchCV cross validation is also performed which is used training the model set."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "vtkrL9MMhB6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model_xgb_gs.feature_importances_)"
      ],
      "metadata": {
        "id": "5eyVx7eIhOSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered R2 , adjusted R2 and RMSE as evaluation metrics.\n",
        "\n",
        "R2 score is a measure of how well the model fits the data.\n",
        "\n",
        "It ranges from 0 to 1, with a higher value indicating a better fit which means that the model is able to explain a large portion of the variance in the data which could have a positive impact on decision-making.\n",
        "\n",
        "\n",
        "RMSE is a measure of the average squared error of the model's predictions.\n",
        "\n",
        "It is calculated as the square root of the mean squared error (MSE).\n",
        "\n",
        "In a business context, a low RMSE can indicate that the model is making relatively small errors."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am considering XGBregressor (xgb_gs) as my final model.\n",
        "\n",
        "This model has the highest R2 and adjusted r2 values on both the training and test sets, which indicates that it is doing a good job of explaining the variance in the target variable and also considering all the features.\n",
        "\n",
        "\n",
        "XGboost capturing all features and still predicting better than ridge. It even has low RMSE among all the three models and also performed well on test data than on the train data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Testing ON Overall Performance\n",
        "\n",
        "Null Hypothesis: The XGBoost model does not have significant predictive power.\n",
        "\n",
        "Alternative Hypothesis: The XGBoost model has significant predictive power.\n"
      ],
      "metadata": {
        "id": "n-2ckQZjlLy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# obtain the predicted values from the XGBoost model\n",
        "y_pred_hypo = xgb_gs.predict(x_test)\n",
        "\n",
        "# calculated the sum of the squared\n",
        "RSS = np.sum((y_test-y_pred_hypo)**2)\n",
        "\n",
        "#calculated the total sum of square\n",
        "TSS = np.sum((y_test-np.mean(y_test))**2)\n",
        "\n",
        "# calculated the degree of the freedom\n",
        "n = len(y_test)\n",
        "k = x_test.shape[1]\n",
        "df_model = k-1\n",
        "df_residual = n-k\n",
        "\n",
        "# calculated the F- statistic\n",
        "F = ((TSS-RSS)/df_model)/(RSS/df_residual)\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - f.cdf(F, df_model, df_residual)\n",
        "\n",
        "# Define the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Perform the hypothesis test\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. The model has significant predictive power.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The model does not have significant predictive power.\")"
      ],
      "metadata": {
        "id": "thuGJOIkmOWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I compareed the F-statistic to the critical value from F distribution to determine wheather to reject the null hypothesis. if P value is less than the choose the siginificance level(alpha). we all the reject the null hypothesis and conclued the model has siginfinance predicitive power."
      ],
      "metadata": {
        "id": "xSRkvJbF3tJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using XGBoost model and for model explainabillity. I am using SHAP(shapley Addicitive Explanation ) values\n",
        "\n",
        "\n",
        "Shapley's value assumes that we can compute the value of the surplus with or without each analyzed factor. The algorithm estimates the value of each factor by assessing the values of its â€˜coalitionsâ€™. In the case of Machine Learning, the â€˜surplusâ€™ is a result of our algorithm and co-operators are different input values. The goal of SHAP is to explain the prediction by computing the contribution of each feature to the final result.\n",
        "\n",
        "I am using TreeExplainer to explain my XGBoost Regressor(xgb_gs)"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "CleUtRMF5_EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(best_model_xgb_gs)\n",
        "\n",
        "# Generate SHAP values for the entire training dataset\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Select a specific record for explanation (e.g., the first record in x_test)\n",
        "\n",
        "\n",
        "# Explain the prediction for the selected record\n",
        "shap.initjs()\n",
        "shap.force_plot(shap_values[0], feature_names=x_independ.columns)"
      ],
      "metadata": {
        "id": "4SsONXBI6PxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0]"
      ],
      "metadata": {
        "id": "mJUi7wjz6c6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "RNUB0UVo6mUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I plotted mean SHAP plot in which feature. we calculated the mean of the absolute values across all observations.\n",
        "\n",
        "There is one bar for each feature.\n",
        "\n",
        "Feature that have larger mean SHAP values will tend to have larger positive/negative  SHAP values. In other words these are the features that  have a significant impact on the model's predicitions.\n",
        "\n",
        "The plot can be used as a feature importance plot to highlight feature that are important to a model's predicition."
      ],
      "metadata": {
        "id": "N0zJPs2j61Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "4RiR77h38rco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "filename = '/content/data_YesBank_StockPrices.csv'\n",
        "pickle.dump(xgb_gs, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "filename = '/content/data_YesBank_StockPrices.csv'\n",
        "yesbank_ml_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc=yesbank_ml_model.predict(x_test)\n",
        "r2_score(y_test,abc)"
      ],
      "metadata": {
        "id": "FnOAtJYp9iWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main goal of the project is to create a machine learning model which can predict the closing price of Yes Bank stock for that month, keeping in mind of the fraud case happened in 2018.\n",
        "\n",
        "I have developed 3 models Ridge_Regression, Random forest and XGBoost Regressor.\n",
        "XGBRegressor model shows promising result with R2 score of 0.97 both on train and test dataset, therefore it can be used to solve this problem. It also considering all newlt added features and taking care of multicollinearity.\n",
        "\n",
        "Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.\n",
        "\n",
        "We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.\n",
        "\n",
        "I considered to take mean of Open, High and Low faetures.Additional features were engineered by taking lags to capture the temporal trends and patterns, which can also can take care of fraud case by studying the pattern.\n",
        "\n",
        "The important feautures which plays a crucial role in deciding closing price are {'OHL', 'lag1', 'lag2', 'lag12','lag9 ', 'lag4', 'lag6', 'lag11'}\n",
        "\n",
        "\n",
        "The dataset has only monthly related price, a daily level price would be more accurate as model can analyze important patterns like week opening price and weekend price. Volume of the data if provided can also be useful in making prediction. A stock prediction involved many aspects like holidays, political decisions, events, un precedented disasters, human decisions. This can be better predicted by having all these features and using time series models like ARIMA and LSTM can ve predicted more accurately.\n",
        "\n",
        "Given the dataset and features, Our model is performing well on all data-points.\n",
        "With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}